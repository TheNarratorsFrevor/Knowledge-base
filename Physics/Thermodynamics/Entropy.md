Entropy is often described as the inherent disorder of a system the more disordered a system is, the higher the entropy of said system.


1. [[Thermodynamics]]:
    
    - [[Boltzmann's entropy formula]]: $S = k \ln W$, where $S$ is the entropy, $k$ is the Boltzmann constant, and $W$ is the number of microstates corresponding to a given macrostate.
    - [[Gibbs entropy formula]]: $S = -k \sum P(i) \ln P(i)$, where $S$ is the entropy, $k$ is the Boltzmann constant, $P(i)$ is the probability of microstate $i$, and the sum is taken over all microstates.
2. [[Information theory]]:
    
    - [[Shannon's entropy formula]]: $H(X) = - \sum P(x) \log P(x)$, where $H(X)$ is the entropy of a discrete random variable $X$, $P(x)$ is the probability of outcome $x$, and the sum is taken over all possible outcomes.
3. [[Statistical mechanics]]:
    
    - [[von Neumann entropy]]: $S = -\mathrm{Tr}(\rho \ln \rho)$, where $S$ is the entropy, $\mathrm{Tr}$ denotes the trace operation, and $\rho$ is the density matrix representing the quantum state.
4. [[Black hole thermodynamics]]:
    
    - [[Bekenstein-Hawking entropy formula]]: $S = \frac{A}{4G}$, where $S$ is the entropy of a black hole, $A$ is its event horizon area, and $G$ is the gravitational constant.
5. [[Cosmology]]:
    
    - [[de Sitter entropy formula]]: $S = \frac{A}{4G}$, where $S$ is the entropy, $A$ is the entangled surface area, and $G$ is the gravitational constant.